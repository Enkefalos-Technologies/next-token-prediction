ðŸš€ Exploring Next Token Prediction in
Theory of Mind (ToM) Task: A
Comparative Experiment with GPT-2 and
Llama-2 Models

ðŸ“Œ Project Overview

This project explores the Theory of Mind using NLP models by predicting next tokens and analyzing how different models perform on first-order, zero-order, and second-order questions.

ðŸ“‚ Project Structure

ðŸ”¹ Data

Original_stories.csv â†’ Contains 10 original stories from the Explore Theory of Mind paper.

infilled_stories.csv â†’ Infills generated using infill_generator_usinggpt4.py.

ðŸ”¹ Code

infill_generator_usinggpt4.py â†’ Script to generate infilled stories.

gpt2_model_pred.py â†’ Predicts next tokens using GPT-2 and plots graphs.

llama2_model_pred.py â†’ Predicts next tokens using LLaMA-2 and plots graphs.

ðŸ”¹ Requirements

requirements.txt â†’ Contains necessary dependencies.

ðŸ”¹ Results (Plotted Graphs)

ðŸ“Š GPT-2 Model Predictions:

gpt2_fo_5march.pdf â†’ First-order question results.

gpt2_so_5march.pdf â†’ Second-order question results.

gpt2_zo_5march.pdf â†’ Zero-order question results.

ðŸ“Š LLaMA-2 Model Predictions:

llama2_fo_5march.pdf â†’ First-order question results.

llama2_so_5march.pdf â†’ Second-order question results.

llama2_zo_5march.pdf â†’ Zero-order question results.

ðŸ›  Installation

To set up the project, follow these steps:

# Clone the repository
git clone https://github.com/Enkefalos-Technologies/next-token-prediction.git
cd your-repo-name

# Install dependencies
pip install -r requirements.txt

ðŸš€ Usage

Run the scripts in the following order:

# Generate infilled stories
python infill_generator_usinggpt4.py or use the  infilled_stories.csv file

# Run GPT-2 model predictions
python gpt2_model_pred.py

# Run LLaMA-2 model predictions
python llama2_model_pred.py

ðŸ“œ License

This project is open-source and available under the MIT License.

![GPT-2 First Order Prediction](gpt2_fo_page1.png)